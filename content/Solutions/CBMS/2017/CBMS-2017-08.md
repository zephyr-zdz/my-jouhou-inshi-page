# CBMS-2017-08

**题目来源**：[[文字版题库/CBMS/2017#Problem 8|2017#Question 8]]
**日期**：2024-07-29
**题目主题**：Math-线性代数-正交投影与特征值

## 解题思路

此问题涉及正交投影、对称矩阵的特征值及矩阵的基向量变换。通过理解正交投影的定义和性质，可以证明所要求的命题。对称矩阵的特征值性质和幺模矩阵的特性，将有助于证明矩阵 $\mathbf{P}$ 的特征值。最后，通过矩阵运算和基变换，可以得到投影矩阵。

## Solution

### Problem (1)

#### (1.1) $T_{\mathbf{a}}(T_{\mathbf{a}}(\mathbf{x})) = T_{\mathbf{a}}(\mathbf{x})$ for any two-dimensional point $\mathbf{x}$

The orthogonal projection of $\mathbf{x}$ on $\mathbf{a}$ is given by:

$$
T_{\mathbf{a}}(\mathbf{x}) = \frac{\mathbf{a} \cdot \mathbf{x}}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a}
$$

To prove the proposition, we apply $T_{\mathbf{a}}$ again on $T_{\mathbf{a}}(\mathbf{x})$:

$$
T_{\mathbf{a}}(T_{\mathbf{a}}(\mathbf{x})) = T_{\mathbf{a}}\left( \frac{\mathbf{a} \cdot \mathbf{x}}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a} \right)
$$

Using the definition of orthogonal projection:

$$
T_{\mathbf{a}}\left( \frac{\mathbf{a} \cdot \mathbf{x}}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a} \right) = \frac{\mathbf{a} \cdot \left( \frac{\mathbf{a} \cdot \mathbf{x}}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a} \right)}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a}
$$

Simplifying the dot products:

$$
= \frac{\frac{(\mathbf{a} \cdot \mathbf{x})(\mathbf{a} \cdot \mathbf{a})}{(\mathbf{a} \cdot \mathbf{a})}}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a} = \frac{\mathbf{a} \cdot \mathbf{x}}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a}
$$

Thus:

$$
T_{\mathbf{a}}(T_{\mathbf{a}}(\mathbf{x})) = T_{\mathbf{a}}(\mathbf{x})
$$

#### (1.2) $T_{\mathbf{b}}(T_{\mathbf{a}}(\mathbf{x})) = \mathbf{o}$ for any non-zero two-dimensional vector $\mathbf{b}$ orthogonal to $\mathbf{a}$

Given $\mathbf{b} \cdot \mathbf{a} = 0$, we need to show:

$$
T_{\mathbf{b}}(T_{\mathbf{a}}(\mathbf{x})) = T_{\mathbf{b}}\left( \frac{\mathbf{a} \cdot \mathbf{x}}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a} \right)
$$

Using the definition of orthogonal projection:

$$
T_{\mathbf{b}}\left( \frac{\mathbf{a} \cdot \mathbf{x}}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a} \right) = \frac{\mathbf{b} \cdot \left( \frac{\mathbf{a} \cdot \mathbf{x}}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a} \right)}{\mathbf{b} \cdot \mathbf{b}} \mathbf{b}
$$

Since $\mathbf{b} \cdot \mathbf{a} = 0$:

$$
= \frac{\frac{(\mathbf{a} \cdot \mathbf{x})(\mathbf{b} \cdot \mathbf{a})}{(\mathbf{a} \cdot \mathbf{a})}}{\mathbf{b} \cdot \mathbf{b}} \mathbf{b} = \frac{(\mathbf{a} \cdot \mathbf{x}) \cdot 0}{(\mathbf{a} \cdot \mathbf{a}) \cdot (\mathbf{b} \cdot \mathbf{b})} \mathbf{b} = 0
$$

Thus:

$$
T_{\mathbf{b}}(T_{\mathbf{a}}(\mathbf{x})) = \mathbf{o}
$$

### Problem (2)

Assume that a real symmetric matrix $\mathbf{P}$ satisfies $\mathbf{P}^2 = \mathbf{P}$. Prove that the eigenvalues of $\mathbf{P}$ are either 0 or 1.

Let $\mathbf{P}$ be a real symmetric matrix. Therefore, it is diagonalizable. Let $\mathbf{v}$ be an eigenvector of $\mathbf{P}$ with eigenvalue $\lambda$:

$$
\mathbf{P}\mathbf{v} = \lambda \mathbf{v}
$$

Applying $\mathbf{P}$ again:

$$
\mathbf{P}^2 \mathbf{v} = \mathbf{P} (\mathbf{P} \mathbf{v}) = \mathbf{P} (\lambda \mathbf{v}) = \lambda \mathbf{P} \mathbf{v} = \lambda (\lambda \mathbf{v}) = \lambda^2 \mathbf{v}
$$

Since $\mathbf{P}^2 = \mathbf{P}$, we have:

$$
\mathbf{P}^2 \mathbf{v} = \mathbf{P} \mathbf{v} = \lambda \mathbf{v}
$$

Thus:

$$
\lambda^2 \mathbf{v} = \lambda \mathbf{v}
$$

Since $\mathbf{v}$ is a non-zero vector, we can conclude:

$$
\lambda^2 = \lambda
$$

Thus, the eigenvalues $\lambda$ must satisfy:

$$
\lambda (\lambda - 1) = 0
$$

Therefore:

$$
\lambda = 0 \quad \text{or} \quad \lambda = 1
$$

### Problem (3)

Given the matrix $\mathbf{A}$ formed by two column vectors $\mathbf{a_1}$ and $\mathbf{a_2}$, which represent the basis of a two-dimensional subspace in three-dimensional space, we want to find the projection matrix $\mathbf{P}$ that projects any vector in $\mathbb{R}^3$ onto this subspace.

Matrix $\mathbf{A}$ is:

$$
\mathbf{A} = [\mathbf{a_1}, \mathbf{a_2}]
$$

where $\mathbf{A}$ is a $3 \times 2$ matrix.

#### Derivation of the Projection Matrix

##### 1. Projection of a Vector

The projection of a vector $\mathbf{x}$ onto the subspace spanned by the columns of $\mathbf{A}$ can be expressed as a linear combination of the columns of $\mathbf{A}$:

$$
\mathbf{P}\mathbf{x} = c_1 \mathbf{a_1} + c_2 \mathbf{a_2}
$$

In matrix form, we write:

$$
\mathbf{P}\mathbf{x} = \mathbf{A}\mathbf{c}
$$

where $\mathbf{c}$ is a column vector of coefficients:

$$
\mathbf{c} = \begin{bmatrix}
c_1 \\
c_2
\end{bmatrix}
$$

##### 2. Finding the Coefficients

To determine the coefficients $\mathbf{c}$, we use the property that the projection minimizes the distance to the subspace. This can be formulated as:

$$
\mathbf{A}^T (\mathbf{x} - \mathbf{A}\mathbf{c}) = \mathbf{0}
$$

This equation implies:

$$
\mathbf{A}^T \mathbf{x} = \mathbf{A}^T \mathbf{A} \mathbf{c}
$$

Assuming $\mathbf{A}^T \mathbf{A}$ is invertible, we solve for $\mathbf{c}$:

$$
\mathbf{c} = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{x}
$$

##### 3. Constructing the Projection Matrix

Substituting $\mathbf{c}$ back into the projection formula, we have:

$$
\mathbf{P}\mathbf{x} = \mathbf{A} (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{x}
$$

Since this holds for any vector $\mathbf{x}$, the projection matrix $\mathbf{P}$ can be identified as:

$$
\mathbf{P} = \mathbf{A} (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T
$$

## 知识点

#对称矩阵 #特征值 #投影矩阵

## 重点词汇

- Orthogonal projection 正交投影
- Symmetric matrix 对称矩阵
- Eigenvalue 特征值
- Column vector 列向量
- Subspace 子空间
- Projection matrix 投影矩阵

## 参考资料

1. Gilbert Strang, "Linear Algebra and Its Applications," Chap. 3, 5.
2. David C. Lay, "Linear Algebra and Its Applications," Chap. 6.
