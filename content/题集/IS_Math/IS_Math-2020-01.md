# IS Math-2022-01

**题目来源**：[[文字版题库/IS_Math/2020#Problem 1]]
**日期**：2024-07-05
**题目主题**：Math-线性代数-矩阵分析

## 具体题目

Square matrices $A$ and $B$ are given by

$$
A = \begin{pmatrix}
1 & \sqrt{2} & 0 \\
\sqrt{2} & 1 & \sqrt{2} \\
0 & \sqrt{2} & 1
\end{pmatrix}, \quad
B = \begin{pmatrix}
0 & -\frac{2}{3} & \frac{1}{3} \\
\frac{2}{3} & 0 & -\frac{2}{3} \\
\frac{1}{3} & \frac{2}{3} & 0
\end{pmatrix}.
$$

For a real square matrix $X$, $\exp(X)$ is defined as follows:

$$
\exp(X) = \sum_{k=0}^{\infty} \left( \frac{1}{k!} X^k \right) = I + X + \frac{1}{2!} X^2 + \frac{1}{3!} X^3 + \cdots,
$$

where $I$ is a unit matrix. Answer the following questions.

1. Calculate all eigenvalues of $A$ and the corresponding eigenvectors whose norm is one and whose first element is a nonnegative real.
2. Calculate $A^n$, where $n$ is a nonnegative integer.
3. Calculate $\exp(A)$.
4. Show that $\exp(\alpha B)$ is represented by the following equation:

$$
\exp(\alpha B) = I + (\sin \alpha) B + (1 - \cos \alpha) B^2,
$$

where $\alpha$ is a real number. You may use the Cayley-Hamilton theorem.

1. A function $f$ of 3-dimensional real vector $x$ is given as follows:

$$
f(x) = \sum_{k=1}^{n} \left\| \exp\left(\frac{2\pi k}{n} B\right) a - x \right\|^2,
$$

where $a$ is a 3-dimensional real vector, and $n \geq 2$. Show that $f$ takes the minimum value at $x = (I + B^2)a$.

## 正确解答

### 1. Eigenvalues and Eigenvectors of $A$

To find the eigenvalues of $A$, we solve the characteristic polynomial $\det(A - \lambda I) = 0$:

$$
A - \lambda I = \begin{pmatrix}
1 - \lambda & \sqrt{2} & 0 \\
\sqrt{2} & 1 - \lambda & \sqrt{2} \\
0 & \sqrt{2} & 1 - \lambda
\end{pmatrix}
$$

The determinant is:

$$
\begin{vmatrix}
1 - \lambda & \sqrt{2} & 0 \\
\sqrt{2} & 1 - \lambda & \sqrt{2} \\
0 & \sqrt{2} & 1 - \lambda
\end{vmatrix} = (1 - \lambda) \left[ (1 - \lambda)^2 - 2 \right] - 2(1 - \lambda) = (1 - \lambda) \left[ \lambda^2 - 2\lambda - 3 \right]
$$

Solving the polynomial:

$$
(1 - \lambda)(\lambda - 3)(\lambda + 1) = 0
$$

The eigenvalues are:

$$
\lambda_1 = 1, \quad \lambda_2 = 3, \quad \lambda_3 = -1
$$

#### Eigenvectors

1. For $\lambda_1 = 1$:

$$
A - I = \begin{pmatrix}
0 & \sqrt{2} & 0 \\
\sqrt{2} & 0 & \sqrt{2} \\
0 & \sqrt{2} & 0
\end{pmatrix} \rightarrow \text{RREF} \rightarrow \begin{pmatrix}
1 & 0 & -1 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{pmatrix}
$$

The eigenvector is:

$$
\mathbf{v}_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix}
$$

1. For $\lambda_2 = 3$:

$$
A - 3I = \begin{pmatrix}
-2 & \sqrt{2} & 0 \\
\sqrt{2} & -2 & \sqrt{2} \\
0 & \sqrt{2} & -2
\end{pmatrix} \rightarrow \text{RREF} \rightarrow \begin{pmatrix}
1 & \sqrt{2} & 1 \\
0 & 1 & \sqrt{2} \\
0 & 0 & 0
\end{pmatrix}
$$

The eigenvector is:

$$
\mathbf{v}_2 = \frac{1}{2} \begin{pmatrix} 1 \\ \sqrt{2} \\ 1 \end{pmatrix}
$$

1. For $\lambda_3 = -1$:

$$
A + I = \begin{pmatrix}
2 & \sqrt{2} & 0 \\
\sqrt{2} & 2 & \sqrt{2} \\
0 & \sqrt{2} & 2
\end{pmatrix} \rightarrow \text{RREF} \rightarrow \begin{pmatrix}
1 & \sqrt{2} & 1 \\
0 & 1 & \sqrt{2} \\
0 & 0 & 0
\end{pmatrix}
$$

The eigenvector is:

$$
\mathbf{v}_3 = \frac{1}{2} \begin{pmatrix} 1 \\ -\sqrt{2} \\ 1 \end{pmatrix}
$$

### 2. Calculating $A^n$

Using the spectral decomposition $A = PDP^{-1}$:

$$
D = \begin{pmatrix}
1 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & -1
\end{pmatrix}, \quad P = \begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{2} & \frac{1}{2} \\
0 & \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\
-\frac{1}{\sqrt{2}} & \frac{1}{2} & \frac{1}{2}
\end{pmatrix}
$$

Thus:

$$
A^n = PD^nP^{-1} = \begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{2} & \frac{1}{2} \\
0 & \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\
-\frac{1}{\sqrt{2}} & \frac{1}{2} & \frac{1}{2}
\end{pmatrix}\begin{pmatrix}
1 & 0 & 0 \\
0 & 3^n & 0 \\
0 & 0 & (-1)^n
\end{pmatrix} \begin{pmatrix}
\frac{1}{\sqrt{2}} & 0 & -\frac{1}{\sqrt{2}} \\
\frac{1}{2} & \frac{\sqrt{2}}{2} & \frac{1}{2} \\
\frac{1}{2} & -\frac{\sqrt{2}}{2} & \frac{1}{2}
\end{pmatrix}
$$

### 3. Calculate $\exp(A)$

To calculate $\exp(A)$, we use the definition of the matrix exponential:

$$
\exp(A) = \sum_{k=0}^{\infty} \frac{1}{k!} A^k = I + A + \frac{1}{2!} A^2 + \frac{1}{3!} A^3 + \cdots
$$

Given the eigenvalues and eigenvectors from the previous steps, we use the spectral decomposition of $A$:

$$
A = PDP^{-1}
$$

where

$$
D = \begin{pmatrix}
1 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & -1
\end{pmatrix}, \quad P = \begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{2} & \frac{1}{2} \\
0 & \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\
-\frac{1}{\sqrt{2}} & \frac{1}{2} & \frac{1}{2}
\end{pmatrix}
$$

Therefore,

$$
A^k = PD^kP^{-1}
$$

Since $D$ is diagonal, $D^k$ is straightforward to calculate:

$$
D^k = \begin{pmatrix}
1^k & 0 & 0 \\
0 & 3^k & 0 \\
0 & 0 & (-1)^k
\end{pmatrix}
$$

Now, compute $\exp(D)$:

$$
\exp(D) = \sum_{k=0}^{\infty} \frac{1}{k!} D^k = \begin{pmatrix}
\exp(1) & 0 & 0 \\
0 & \exp(3) & 0 \\
0 & 0 & \exp(-1)
\end{pmatrix}
$$

Thus,

$$
\exp(A) = P \exp(D) P^{-1}
$$

Compute $\exp(D)$:

$$
\exp(D) = \begin{pmatrix}
e & 0 & 0 \\
0 & e^3 & 0 \\
0 & 0 & \frac{1}{e}
\end{pmatrix}
$$

Therefore,

$$
\exp(A) = P \begin{pmatrix}
e & 0 & 0 \\
0 & e^3 & 0 \\
0 & 0 & \frac{1}{e}
\end{pmatrix} P^{-1}
$$

We already have $P$ and $P^{-1}$. Calculate $P^{-1}$:

$$
P^{-1} = \begin{pmatrix}
\frac{1}{\sqrt{2}} & 0 & -\frac{1}{\sqrt{2}} \\
\frac{1}{2} & \frac{\sqrt{2}}{2} & \frac{1}{2} \\
\frac{1}{2} & -\frac{\sqrt{2}}{2} & \frac{1}{2}
\end{pmatrix}
$$

Multiply the matrices:

$$
\exp(A) = \begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{2} & \frac{1}{2} \\
0 & \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\
-\frac{1}{\sqrt{2}} & \frac{1}{2} & \frac{1}{2}
\end{pmatrix}
\begin{pmatrix}
e & 0 & 0 \\
0 & e^3 & 0 \\
0 & 0 & \frac{1}{e}
\end{pmatrix}
\begin{pmatrix}
\frac{1}{\sqrt{2}} & 0 & -\frac{1}{\sqrt{2}} \\
\frac{1}{2} & \frac{\sqrt{2}}{2} & \frac{1}{2} \\
\frac{1}{2} & -\frac{\sqrt{2}}{2} & \frac{1}{2}
\end{pmatrix}
$$

### 4. Showing $\exp(\alpha B)$

Using the **Cayley-Hamilton theorem**, the characteristic polynomial of $B$ gives $B^3 - (\text{tr}(B))B^2 + (\text{sum of principal minors})B - \det(B)I = 0$. Since $B$ is traceless and its determinant is zero, we simplify to $B^3 = -B$.

The matrix exponential $\exp(\alpha B)$ is given by:

$$
\exp(\alpha B) = \sum_{k=0}^{\infty} \frac{(\alpha B)^k}{k!}
$$

Given the properties of $B$, we can write:

$$
\exp(\alpha B) = I + \alpha B + \frac{(\alpha B)^2}{2!} + \frac{(\alpha B)^3}{3!} + \cdots
$$

Since $B^3 = -B$, higher powers of $B$ can be expressed in terms of $B$ and $B^2$. The series becomes:

$$
\exp(\alpha B) = I + \left( \alpha - \frac{\alpha^3}{3!} + \frac{\alpha^5}{5!} - \cdots \right) B + \left( \frac{\alpha^2}{2!} - \frac{\alpha^4}{4!} + \cdots \right) B^2
$$

Recognizing the Taylor series for $\sin(\alpha)$ and $1 - \cos(\alpha)$:

$$
\sin(\alpha) = \alpha - \frac{\alpha^3}{3!} + \frac{\alpha^5}{5!} - \cdots
$$

$$
1 - \cos(\alpha) = \frac{\alpha^2}{2!} - \frac{\alpha^4}{4!} + \cdots
$$

Thus,

$$
\exp(\alpha B) = I + (\sin \alpha) B + (1 - \cos \alpha) B^2
$$

### 5. Minimizing $f(x)$

Given the function:

$$
f(x) = \sum_{k=1}^{n} \left\| \exp\left(\frac{2\pi k}{n} B\right) a - x \right\|^2
$$

where $a$ is a 3-dimensional real vector, and $n \geq 2$.

First, we express the exponential term $\exp\left(\frac{2\pi k}{n} B\right)$. From the previous question, we know:

$$
\exp(\alpha B) = I + (\sin \alpha) B + (1 - \cos \alpha) B^2
$$

Therefore, for $\alpha = \frac{2\pi k}{n}$:

$$
\exp\left(\frac{2\pi k}{n} B\right) = I + \sin\left(\frac{2\pi k}{n}\right) B + \left(1 - \cos\left(\frac{2\pi k}{n}\right)\right) B^2
$$

Substitute this into the function $f(x)$:

$$
f(x) = \sum_{k=1}^{n} \left\| \left(I + \sin\left(\frac{2\pi k}{n}\right) B + \left(1 - \cos\left(\frac{2\pi k}{n}\right)\right) B^2 \right) a - x \right\|^2
$$

Now, let's denote $\exp\left(\frac{2\pi k}{n} B\right) a$ as $y_k$:

$$
y_k = \left(I + \sin\left(\frac{2\pi k}{n}\right) B + \left(1 - \cos\left(\frac{2\pi k}{n}\right)\right) B^2 \right) a
$$

Thus, $f(x)$ becomes:

$$
f(x) = \sum_{k=1}^{n} \left\| y_k - x \right\|^2
$$

We need to minimize $f(x)$ with respect to $x$. To do this, we find the critical point by setting the gradient $\nabla f(x)$ to zero.

The gradient of $f(x)$ with respect to $x$ is:

$$
\nabla f(x) = -2 \sum_{k=1}^{n} (y_k - x)
$$

Set the gradient to zero:

$$
-2 \sum_{k=1}^{n} (y_k - x) = 0
$$

$$
\sum_{k=1}^{n} (y_k - x) = 0
$$

$$
\sum_{k=1}^{n} y_k = n x
$$

Therefore, the minimizing $x$ is given by:

$$
x = \frac{1}{n} \sum_{k=1}^{n} y_k
$$

Now, we compute the sum $\sum_{k=1}^{n} y_k$:

$$
\sum_{k=1}^{n} y_k = \sum_{k=1}^{n} \left(I + \sin\left(\frac{2\pi k}{n}\right) B + \left(1 - \cos\left(\frac{2\pi k}{n}\right)\right) B^2 \right) a
$$

Notice that the identity matrix $I$ and the vector $a$ are constants, so we can factor them out:

$$
\sum_{k=1}^{n} y_k = \left(\sum_{k=1}^{n} I \right) a + \left(\sum_{k=1}^{n} \sin\left(\frac{2\pi k}{n}\right) B \right) a + \left(\sum_{k=1}^{n} \left(1 - \cos\left(\frac{2\pi k}{n}\right)\right) B^2 \right) a
$$

We simplify each term separately:

1. For the first term:

$$
\sum_{k=1}^{n} I = n I
$$

1. For the second term, consider the summation of sine terms:

$$
\sum_{k=1}^{n} \sin\left(\frac{2\pi k}{n}\right)
$$

Since $\sin \left( \frac{2\pi k}{n} \right)$ is symmetric around the unit circle, the sum of sines over one period $2\pi$ is zero:

$$
\sum_{k=1}^{n} \sin\left(\frac{2\pi k}{n}\right) = 0
$$

1. For the third term, consider the summation of cosine terms:

$$
\sum_{k=1}^{n} \left(1 - \cos\left(\frac{2\pi k}{n}\right)\right)
$$

Since $\cos \left( \frac{2\pi k}{n} \right)$ is also symmetric around the unit circle, the sum of cosines over one period $2\pi$ is zero, and thus:

$$
\sum_{k=1}^{n} \left(1 - \cos\left(\frac{2\pi k}{n}\right)\right) = \sum_{k=1}^{n} 1 - \sum_{k=1}^{n} \cos\left(\frac{2\pi k}{n}\right) = n - 0 = n
$$

So, the third term simplifies to:

$$
n B^2
$$

Combining these results:

$$
\sum_{k=1}^{n} y_k = n I a + n B^2 a = n (I + B^2) a
$$

Thus,

$$
x = \frac{1}{n} \sum_{k=1}^{n} y_k = \frac{1}{n} n (I + B^2) a = (I + B^2) a
$$

Therefore, the function $f(x)$ takes the minimum value at:

$$
x = (I + B^2) a
$$

### Convexity of $f(x)$

To show that $f(x)$ is convex, we need to prove that its Hessian is positive semi-definite. Recall that:

$$
f(x) = \sum_{k=1}^{n} \left\| y_k - x \right\|^2
$$

We expand this:

$$
f(x) = \sum_{k=1}^{n} (y_k - x)^T (y_k - x) = \sum_{k=1}^{n} (y_k^T y_k - 2 y_k^T x + x^T x)
$$

Since $y_k^T y_k$ is a constant and $\sum_{k=1}^{n} y_k$ is a vector, the quadratic term $x^T x$ dominates the expression. The Hessian of $f(x)$ is computed as:

$$
H = \nabla^2 f(x) = 2n I
$$

Since the Hessian is a scaled identity matrix, it is positive definite. Therefore, $f(x)$ is a convex function.

## 知识点

#线性代数 #特征值 #特征向量 #矩阵指数 #Cayley-Hamilton定理 #凸函数

- [[Matrix Theory#Cayley-Hamilton 定理 Cayley-Hamilton Theorem]]
- [[Linear Equations (Ax=0 & Ax=b)#齐次线性方程组 $Ax = 0$ Homogeneous System $Ax = 0$]]
- [[Eigenvalue & Eigen vector#3. 计算方法 (Computation Methods)]]
- [[Expansions#泰勒展开 Taylor Expansion]]
- [[Convex Optimization]]

## 解题技巧和信息

- 使用特征值分解简化矩阵幂和指数的计算
- Cayley-Hamilton 定理用于简化矩阵多项式
- 凸函数的性质帮助确定最优解的存在性和唯一性

## 重点词汇

eigenvalue 特征值

eigenvector 特征向量

matrix exponential 矩阵指数

Cayley-Hamilton theorem Cayley-Hamilton 定理

convex function 凸函数
