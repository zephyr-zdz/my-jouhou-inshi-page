# IS CS-2021W-04

**题目来源**：[[2021W#Problem 4]]
**日期**：2024-07-24
**题目主题**：CS-算法-最优化问题与线性代数

## 解题思路

这道题目涉及通过拉格朗日乘数法解决带有约束条件的优化问题，并利用奇异值分解（SVD）简化表达式。主要步骤包括：

1. 使用拉格朗日乘数法解约束优化问题。
2. 使用奇异值分解简化矩阵表达式。
3. 根据优化问题的 KKT 条件表达解的形式。

## Solution

### Question 1: Express $\mathbf{XX^-X}$ using only $\mathbf{X}$

Given:

$$
\mathbf{X} = \mathbf{U} \mathbf{\Lambda} \mathbf{V}^\top,
$$

$$
\mathbf{X^-} = \mathbf{V} \mathbf{\Lambda}^- \mathbf{U}^\top,
$$

we can write:

$$
\mathbf{X^-} = \mathbf{V} (\mathbf{\Lambda}^-) \mathbf{U}^\top,
$$

where

$$
\mathbf{\Lambda}^- = \begin{pmatrix}
\frac{1}{\lambda_1} & 0 & \cdots & 0 & 0 \\
0 & \frac{1}{\lambda_2} & \cdots & 0 & 0 \\
0 & 0 & \ddots & 0 & 0 \\
0 & 0 & \cdots & \frac{1}{\lambda_n} & 0 \\
\end{pmatrix}.
$$

To find $\mathbf{XX^-X}$:

$$
\mathbf{XX^-X} = (\mathbf{U} \mathbf{\Lambda} \mathbf{V}^\top)(\mathbf{V} \mathbf{\Lambda}^- \mathbf{U}^\top)(\mathbf{U} \mathbf{\Lambda} \mathbf{V}^\top).
$$

Notice:

$$
\mathbf{V}^\top \mathbf{V} = \mathbf{I}_d, \quad \mathbf{U}^\top \mathbf{U} = \mathbf{I}_n,
$$

therefore:

$$
\mathbf{XX^-X} = \mathbf{U} \mathbf{\Lambda} (\mathbf{V}^\top \mathbf{V}) (\mathbf{\Lambda}^-) \mathbf{U}^\top \mathbf{U} \mathbf{\Lambda} \mathbf{V}^\top = \mathbf{U} \mathbf{\Lambda} (\mathbf{\Lambda}^-) \mathbf{\Lambda} \mathbf{V}^\top = \mathbf{U} \mathbf{\Lambda} \mathbf{I}_n \mathbf{\Lambda} \mathbf{V}^\top = \mathbf{U} \mathbf{\Lambda} \mathbf{\Lambda} \mathbf{V}^\top = \mathbf{X}.
$$

Thus,

$$
\boxed{\mathbf{XX^-X} = \mathbf{X}}.
$$

### Question 2: Express $\mathbf{XX^\top}$ using only $\mathbf{U}$ and $\lambda_i$ ($i = 1, \ldots, n$)

Given:

$$
\mathbf{X} = \mathbf{U} \mathbf{\Lambda} \mathbf{V}^\top,
$$

we have:

$$
\mathbf{XX^\top} = (\mathbf{U} \mathbf{\Lambda} \mathbf{V}^\top)(\mathbf{V} \mathbf{\Lambda}^\top \mathbf{U}^\top) = \mathbf{U} \mathbf{\Lambda} \mathbf{\Lambda}^\top \mathbf{U}^\top.
$$

Since $\mathbf{\Lambda}$ is an $n \times d$ matrix with singular values $\lambda_1, \lambda_2, \ldots, \lambda_n$ on the diagonal, $\mathbf{\Lambda} \mathbf{\Lambda}^\top$ is an $n \times n$ diagonal matrix:

$$
\mathbf{\Lambda} \mathbf{\Lambda}^\top = \mathrm{diag}(\lambda_1^2, \lambda_2^2, \ldots, \lambda_n^2).
$$

Therefore,

$$
\mathbf{XX^\top} = \mathbf{U} \begin{pmatrix}
\lambda_1^2 & 0 & \cdots & 0 \\
0 & \lambda_2^2 & \cdots & 0 \\
0 & 0 & \ddots & 0 \\
0 & 0 & \cdots & \lambda_n^2 \\
\end{pmatrix} \mathbf{U}^\top.
$$

Thus,

$$
\boxed{\mathbf{XX^\top} = \mathbf{U} \mathrm{diag}(\lambda_1^2, \lambda_2^2, \ldots, \lambda_n^2) \mathbf{U}^\top}.
$$

### Question 3: Express the stationary points of $L(\mathbf{w}, \boldsymbol{\mu})$ in the form of $\mathbf{w} = \mathbf{A} \mathbf{y}$ and $\boldsymbol{\mu} = \mathbf{B} \mathbf{y}$

To solve the optimization problem using Lagrange multipliers:

$$
L(\mathbf{w}, \boldsymbol{\mu}) = \frac{1}{2} \|\mathbf{w}\|^2 + \boldsymbol{\mu}^\top (\mathbf{y} - \mathbf{Xw}),
$$

we need to find $\mathbf{w}$ and $\boldsymbol{\mu}$ such that:

$$
\frac{\partial L}{\partial \mathbf{w}} = 0 \quad \text{and} \quad \frac{\partial L}{\partial \boldsymbol{\mu}} = 0.
$$

First, compute $\frac{\partial L}{\partial \mathbf{w}}$:

$$
\frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} - \mathbf{X}^\top \boldsymbol{\mu} = 0 \implies \mathbf{w} = \mathbf{X}^\top \boldsymbol{\mu}.
$$

Next, compute $\frac{\partial L}{\partial \boldsymbol{\mu}}$:

$$
\frac{\partial L}{\partial \boldsymbol{\mu}} = \mathbf{y} - \mathbf{Xw} = 0 \implies \mathbf{y} = \mathbf{Xw}.
$$

Substituting $\mathbf{w} = \mathbf{X}^\top \boldsymbol{\mu}$ into $\mathbf{y} = \mathbf{Xw}$:

$$
\mathbf{y} = \mathbf{X} (\mathbf{X}^\top \boldsymbol{\mu}) \implies \mathbf{y} = (\mathbf{X} \mathbf{X}^\top) \boldsymbol{\mu}.
$$

Since $\mathbf{X} \mathbf{X}^\top$ is invertible,

$$
\boldsymbol{\mu} = (\mathbf{X} \mathbf{X}^\top)^{-1} \mathbf{y}.
$$

Then,

$$
\mathbf{w} = \mathbf{X}^\top \boldsymbol{\mu} = \mathbf{X}^\top (\mathbf{X} \mathbf{X}^\top)^{-1} \mathbf{y}.
$$

Therefore,

$$
\mathbf{A} = \mathbf{X}^\top (\mathbf{X} \mathbf{X}^\top)^{-1}, \quad \mathbf{B} = (\mathbf{X} \mathbf{X}^\top)^{-1}.
$$

Thus,

$$
\boxed{\mathbf{w} = \mathbf{X}^\top (\mathbf{X} \mathbf{X}^\top)^{-1} \mathbf{y}, \quad \boldsymbol{\mu} = (\mathbf{X} \mathbf{X}^\top)^{-1} \mathbf{y}}.
$$

### Question 4: Express $\mathbf{A}$ using only $\mathbf{X^-}$

From question 3, we have:

$$
\mathbf{A} = \mathbf{X}^\top (\mathbf{X} \mathbf{X}^\top)^{-1}.
$$

Using $\mathbf{X^-}$:

$$
\mathbf{X^-} = \mathbf{V} \mathbf{\Lambda}^- \mathbf{U}^\top,
$$

we know:

$$
\mathbf{X} \mathbf{X}^- = \mathbf{U} \mathbf{\Lambda} \mathbf{V}^\top \mathbf{V} \mathbf{\Lambda}^- \mathbf{U}^\top = \mathbf{U} \mathbf{\Lambda} \mathbf{\Lambda}^-\mathbf{U}^\top = \mathbf{U} \mathbf{I}_n \mathbf{U}^\top = \mathbf{I}_n,
$$

Therefore:

$$
\mathbf{X}^\top (\mathbf{X} \mathbf{X}^\top)^{-1} = \mathbf{X^-}.
$$

Another way to see this is to use the given SVD of $\mathbf{X}$ and $\mathbf{X}\mathbf{X}^\top$ from Question 2:

$$
\mathbf{X} = \mathbf{U} \mathbf{\Lambda} \mathbf{V}^\top,
$$

$$
\mathbf{X}\mathbf{X}^\top = \mathbf{U} \mathrm{diag}(\lambda_1^2, \lambda_2^2, \ldots, \lambda_n^2) \mathbf{U}^\top
$$

Then,

$$
\mathbf{A} = \mathbf{X}^\top (\mathbf{X} \mathbf{X}^\top)^{-1}
$$

$$
= \mathbf{V} \mathbf{\Lambda}^\top \mathbf{U}^\top (\mathbf{U} \mathrm{diag}(\lambda_1^2, \lambda_2^2, \ldots, \lambda_n^2) \mathbf{U}^\top)^{-1}
$$

$$
= \mathbf{V} \mathbf{\Lambda}^\top \mathbf{U}^\top \mathbf{U} \mathrm{diag}(\lambda_1^{-2}, \lambda_2^{-2}, \ldots, \lambda_n^{-2}) \mathbf{U}^\top
$$

$$
= \mathbf{V} \mathbf{\Lambda}^\top \mathrm{diag}(\lambda_1^{-2}, \lambda_2^{-2}, \ldots, \lambda_n^{-2}) \mathbf{U}^\top
$$

$$
= \mathbf{V} \begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
0 & 0 & \ddots & 0 \\
0 & 0 & \cdots & \lambda_n \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & 0
\end{pmatrix} \begin{pmatrix}
\lambda_1^{-2} & 0 & \cdots & 0 \\
0 & \lambda_2^{-2} & \cdots & 0 \\
0 & 0 & \ddots & 0 \\
0 & 0 & \cdots & \lambda_n^{-2} \\
\end{pmatrix} \mathbf{U}^\top
$$

$$
= \mathbf{V} \begin{pmatrix}
\lambda_1^{-1} & 0 & \cdots & 0 \\
0 & \lambda_2^{-1} & \cdots & 0 \\
0 & 0 & \ddots & 0 \\
0 & 0 & \cdots & \lambda_n^{-1} \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & 0
\end{pmatrix} \mathbf{U}^\top
= \mathbf{X^-}.
$$

Thus,

$$
\boxed{\mathbf{A} = \mathbf{X^-}}.
$$

## 知识点

#最优化 #奇异值分解 #线性代数 #拉格朗日乘数法

## 难点思路

这道题目涉及多个知识点的综合运用，特别是拉格朗日乘数法和奇异值分解的结合使用。重点在于理解矩阵运算和变换的基本性质。

## 解题技巧和信息

- 拉格朗日乘数法：在求解带有约束的最优化问题时非常有用。
- 奇异值分解：帮助简化矩阵运算，特别是对于逆矩阵和伪逆矩阵的计算。

## 重点词汇

- inner product 内积
- transpose 转置
- Lagrange multipliers 拉格朗日乘数
- singular value decomposition 奇异值分解
- orthonormal basis 正交归一基

## 参考资料

1. 《线性代数及其应用》David C. Lay，第四章：奇异值分解
2. 《最优化理论》Edwin K. P. Chong and Stanislaw H. Zak，第三章：拉格朗日乘数法
